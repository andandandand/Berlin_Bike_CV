{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graumannm/Berlin_Bike_CV/blob/main/s01_extract_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This notebook loads a model and runs a foward pass of all images in our created bike lane dataset to extract the embeddings from a pre-defined model layer"
      ],
      "metadata": {
        "id": "qPmH7quoBX6B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Np7i7fddhkj0"
      },
      "outputs": [],
      "source": [
        "# define model and dataset names\n",
        "model_name = 'dinoS14'\n",
        "dataset = 'bikelanes'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "XlnRnL74AsmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import time\n",
        "from PIL import Image\n",
        "import skimage.io as io\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Pf7v8dHfAuI9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR4wIrS-dTx4",
        "outputId": "10550a31-74f4-4d58-b1aa-76e6a346cc64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "# connect to drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for picking a model and its corresponding layer"
      ],
      "metadata": {
        "id": "l3vZtedBB9vK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lhgcmeGMezf5"
      },
      "outputs": [],
      "source": [
        "# function for picking a model\n",
        "def pick_model(model_name):\n",
        "\n",
        "    if model_name == 'resnet':\n",
        "\n",
        "      print('loading ResNet')\n",
        "\n",
        "      # load model\n",
        "      model_path = '/gdrive/MyDrive/berlin_bike_CV/CobblestoneModel/finetuned_ResNet101.pt'\n",
        "      model = torch.load(model_path)\n",
        "\n",
        "      # select layer for feature extraction\n",
        "      my_layer = model.avgpool\n",
        "\n",
        "    elif model_name == 'mask2former':\n",
        "\n",
        "      print('loading Mask2Former') # image processing for mask2former not implemented yet\n",
        "\n",
        "      # load model\n",
        "      !pip install -q git+https://github.com/huggingface/transformers.git\n",
        "      from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
        "      model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-mapillary-vistas-semantic\")\n",
        "\n",
        "      # select layer for feature extraction\n",
        "      my_layer = model.model.pixel_level_module.encoder.encoder.layers[3].blocks[1].output\n",
        "\n",
        "    elif model_name == 'dinoS14':\n",
        "\n",
        "      print('loading DinoV2 S14')\n",
        "\n",
        "      # load model\n",
        "      model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "\n",
        "      # select layer for feature extraction\n",
        "      my_layer = model.head\n",
        "\n",
        "    elif model_name == 'dinoG14':\n",
        "\n",
        "      print('loading DinoV2 G14')\n",
        "\n",
        "      # load model\n",
        "      model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
        "\n",
        "      # select layer for feature extraction\n",
        "      my_layer = model.head\n",
        "\n",
        "    return model, my_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D_JpHlj2he2o"
      },
      "outputs": [],
      "source": [
        "# define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224), # new size will be 3x224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydkyWXn4hiBb",
        "outputId": "c36ef8ae-3d25-4b14-ad86-48f47f1069e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading DinoV2 S14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "WARNING:dinov2:xFormers not available\n",
            "WARNING:dinov2:xFormers not available\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
            "100%|██████████| 84.2M/84.2M [00:04<00:00, 20.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# load model and define layer for hook\n",
        "model, my_layer = pick_model(model_name)\n",
        "\n",
        "# put model in evaluation mode for consistent results\n",
        "model.eval()\n",
        "\n",
        "# deactivate gradients\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create hook on my_layer to get features\n",
        "features = []\n",
        "def hook(module, input, output):\n",
        "    features.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run foward pass on all image to extract embeddings"
      ],
      "metadata": {
        "id": "9jyilw2rBB8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGxXBUl1eShy",
        "outputId": "c1586828-ca90-4c23-da91-44ffba7229aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image # 1  out of  242\n",
            "Image # 2  out of  242\n",
            "Image # 3  out of  242\n",
            "Image # 4  out of  242\n",
            "Image # 5  out of  242\n",
            "Image # 6  out of  242\n",
            "Image # 7  out of  242\n",
            "Image # 8  out of  242\n",
            "Image # 9  out of  242\n",
            "Image # 10  out of  242\n",
            "Image # 11  out of  242\n",
            "Image # 12  out of  242\n",
            "Image # 13  out of  242\n",
            "Image # 14  out of  242\n",
            "Image # 15  out of  242\n",
            "Image # 16  out of  242\n",
            "Image # 17  out of  242\n",
            "Image # 18  out of  242\n",
            "Image # 19  out of  242\n",
            "Image # 20  out of  242\n",
            "Image # 21  out of  242\n",
            "Image # 22  out of  242\n",
            "Image # 23  out of  242\n",
            "Image # 24  out of  242\n",
            "Image # 25  out of  242\n",
            "Image # 26  out of  242\n",
            "Image # 27  out of  242\n",
            "Image # 28  out of  242\n",
            "Image # 29  out of  242\n",
            "Image # 30  out of  242\n",
            "Image # 31  out of  242\n",
            "Image # 32  out of  242\n",
            "Image # 33  out of  242\n",
            "Image # 34  out of  242\n",
            "Image # 35  out of  242\n",
            "Image # 36  out of  242\n",
            "Image # 37  out of  242\n",
            "Image # 38  out of  242\n",
            "Image # 39  out of  242\n",
            "Image # 40  out of  242\n",
            "Image # 41  out of  242\n",
            "Image # 42  out of  242\n",
            "Image # 43  out of  242\n",
            "Image # 44  out of  242\n",
            "Image # 45  out of  242\n",
            "Image # 46  out of  242\n",
            "Image # 47  out of  242\n",
            "Image # 48  out of  242\n",
            "Image # 49  out of  242\n",
            "Image # 50  out of  242\n",
            "Image # 51  out of  242\n",
            "Image # 52  out of  242\n",
            "Image # 53  out of  242\n",
            "Image # 54  out of  242\n",
            "Image # 55  out of  242\n",
            "Image # 56  out of  242\n",
            "Image # 57  out of  242\n",
            "Image # 58  out of  242\n",
            "Image # 59  out of  242\n",
            "Image # 60  out of  242\n",
            "Image # 61  out of  242\n",
            "Image # 62  out of  242\n",
            "Image # 63  out of  242\n",
            "Image # 64  out of  242\n",
            "Image # 65  out of  242\n",
            "Image # 66  out of  242\n",
            "Image # 67  out of  242\n",
            "Image # 68  out of  242\n",
            "Image # 69  out of  242\n",
            "Image # 70  out of  242\n",
            "Image # 71  out of  242\n",
            "Image # 72  out of  242\n",
            "Image # 73  out of  242\n",
            "Image # 74  out of  242\n",
            "Image # 75  out of  242\n",
            "Image # 76  out of  242\n",
            "Image # 77  out of  242\n",
            "Image # 78  out of  242\n",
            "Image # 79  out of  242\n",
            "Image # 80  out of  242\n",
            "Image # 81  out of  242\n",
            "Image # 82  out of  242\n",
            "Image # 83  out of  242\n",
            "Image # 84  out of  242\n",
            "Image # 85  out of  242\n",
            "Image # 86  out of  242\n",
            "Image # 87  out of  242\n",
            "Image # 88  out of  242\n",
            "Image # 89  out of  242\n",
            "Image # 90  out of  242\n",
            "Image # 91  out of  242\n",
            "Image # 92  out of  242\n",
            "Image # 93  out of  242\n",
            "Image # 94  out of  242\n",
            "Image # 95  out of  242\n",
            "Image # 96  out of  242\n",
            "Image # 97  out of  242\n",
            "Image # 98  out of  242\n",
            "Image # 99  out of  242\n",
            "Image # 100  out of  242\n",
            "Image # 101  out of  242\n",
            "Image # 102  out of  242\n",
            "Image # 103  out of  242\n",
            "Image # 104  out of  242\n",
            "Image # 105  out of  242\n",
            "Image # 106  out of  242\n",
            "Image # 107  out of  242\n",
            "Image # 108  out of  242\n",
            "Image # 109  out of  242\n",
            "Image # 110  out of  242\n",
            "Image # 111  out of  242\n",
            "Image # 112  out of  242\n",
            "Image # 113  out of  242\n",
            "Image # 114  out of  242\n",
            "Image # 115  out of  242\n",
            "Image # 116  out of  242\n",
            "Image # 117  out of  242\n",
            "Image # 118  out of  242\n",
            "Image # 119  out of  242\n",
            "Image # 120  out of  242\n",
            "Image # 121  out of  242\n",
            "Image # 122  out of  242\n",
            "Image # 123  out of  242\n",
            "Image # 124  out of  242\n",
            "Image # 125  out of  242\n",
            "Image # 126  out of  242\n",
            "Image # 127  out of  242\n",
            "Image # 128  out of  242\n",
            "Image # 129  out of  242\n",
            "Image # 130  out of  242\n",
            "Image # 131  out of  242\n",
            "Image # 132  out of  242\n",
            "Image # 133  out of  242\n",
            "Image # 134  out of  242\n",
            "Image # 135  out of  242\n",
            "Image # 136  out of  242\n",
            "Image # 137  out of  242\n",
            "Image # 138  out of  242\n",
            "Image # 139  out of  242\n",
            "Image # 140  out of  242\n",
            "Image # 141  out of  242\n",
            "Image # 142  out of  242\n",
            "Image # 143  out of  242\n",
            "Image # 144  out of  242\n",
            "Image # 145  out of  242\n",
            "Image # 146  out of  242\n",
            "Image # 147  out of  242\n",
            "Image # 148  out of  242\n",
            "Image # 149  out of  242\n",
            "Image # 150  out of  242\n",
            "Image # 151  out of  242\n",
            "Image # 152  out of  242\n",
            "Image # 153  out of  242\n",
            "Image # 154  out of  242\n",
            "Image # 155  out of  242\n",
            "Image # 156  out of  242\n",
            "Image # 157  out of  242\n",
            "Image # 158  out of  242\n",
            "Image # 159  out of  242\n",
            "Image # 160  out of  242\n",
            "Image # 161  out of  242\n",
            "Image # 162  out of  242\n",
            "Image # 163  out of  242\n",
            "Image # 164  out of  242\n",
            "Image # 165  out of  242\n",
            "Image # 166  out of  242\n",
            "Image # 167  out of  242\n",
            "Image # 168  out of  242\n",
            "Image # 169  out of  242\n",
            "Image # 170  out of  242\n",
            "Image # 171  out of  242\n",
            "Image # 172  out of  242\n",
            "Image # 173  out of  242\n",
            "Image # 174  out of  242\n",
            "Image # 175  out of  242\n",
            "Image # 176  out of  242\n",
            "Image # 177  out of  242\n",
            "Image # 178  out of  242\n",
            "Image # 179  out of  242\n",
            "Image # 180  out of  242\n",
            "Image # 181  out of  242\n",
            "Image # 182  out of  242\n",
            "Image # 183  out of  242\n",
            "Image # 184  out of  242\n",
            "Image # 185  out of  242\n",
            "Image # 186  out of  242\n",
            "Image # 187  out of  242\n",
            "Image # 188  out of  242\n",
            "Image # 189  out of  242\n",
            "Image # 190  out of  242\n",
            "Image # 191  out of  242\n",
            "Image # 192  out of  242\n",
            "Image # 193  out of  242\n",
            "Image # 194  out of  242\n",
            "Image # 195  out of  242\n",
            "Image # 196  out of  242\n",
            "Image # 197  out of  242\n",
            "Image # 198  out of  242\n",
            "Image # 199  out of  242\n",
            "Image # 200  out of  242\n",
            "Image # 201  out of  242\n",
            "Image # 202  out of  242\n",
            "Image # 203  out of  242\n",
            "Image # 204  out of  242\n",
            "Image # 205  out of  242\n",
            "Image # 206  out of  242\n",
            "Image # 207  out of  242\n",
            "Image # 208  out of  242\n",
            "Image # 209  out of  242\n",
            "Image # 210  out of  242\n",
            "Image # 211  out of  242\n",
            "Image # 212  out of  242\n",
            "Image # 213  out of  242\n",
            "Image # 214  out of  242\n",
            "Image # 215  out of  242\n",
            "Image # 216  out of  242\n",
            "Image # 217  out of  242\n",
            "Image # 218  out of  242\n",
            "Image # 219  out of  242\n",
            "Image # 220  out of  242\n",
            "Image # 221  out of  242\n",
            "Image # 222  out of  242\n",
            "Image # 223  out of  242\n",
            "Image # 224  out of  242\n",
            "Image # 225  out of  242\n",
            "Image # 226  out of  242\n",
            "Image # 227  out of  242\n",
            "Image # 228  out of  242\n",
            "Image # 229  out of  242\n",
            "Image # 230  out of  242\n",
            "Image # 231  out of  242\n",
            "Image # 232  out of  242\n",
            "Image # 233  out of  242\n",
            "Image # 234  out of  242\n",
            "Image # 235  out of  242\n",
            "Image # 236  out of  242\n",
            "Image # 237  out of  242\n",
            "Image # 238  out of  242\n",
            "Image # 239  out of  242\n",
            "Image # 240  out of  242\n",
            "Image # 241  out of  242\n",
            "Image # 242  out of  242\n",
            "Execution time: 55.70071363449097 seconds\n"
          ]
        }
      ],
      "source": [
        "# get the start time\n",
        "st = time.time()\n",
        "\n",
        "# define data paths\n",
        "img_dir  = '/gdrive/My Drive/berlin_bike_CV/final_project_first_images/labelled images/'\n",
        "# initialize empty dict\n",
        "img_embedding  = {}\n",
        "full_img_paths = {}\n",
        "counter = 0\n",
        "\n",
        "image_files = image_files = os.listdir(img_dir + 'bikelanes/')\n",
        "\n",
        "for i, image_file in enumerate(image_files):\n",
        "\n",
        "    # progress report\n",
        "    print('Image #', i+1, ' out of ',len(image_files))\n",
        "\n",
        "    # load current image as PIL.Image.Image\n",
        "    img = Image.open(img_dir + 'bikelanes/' + image_files[i])\n",
        "\n",
        "    # transform image\n",
        "    img_t = transform(img)\n",
        "\n",
        "    # run foward pass\n",
        "    img_unsqueezed = torch.unsqueeze(img_t, 0) # add first singleton dimension, the 'batch'\n",
        "    features = []  # Reset features list for each image\n",
        "    hook_handle = my_layer.register_forward_hook(hook)  # Register the hook for the current layer\n",
        "    out = model(img_unsqueezed)\n",
        "    hook_handle.remove()  # Remove the hook after extracting features\n",
        "\n",
        "    # extract features, convert to np array and save with file name as key in dictionary\n",
        "    img_embedding[image_files[i]] = torch.squeeze(torch.flatten(features[0])).numpy()\n",
        "\n",
        "    # make another dictionary with full path as value\n",
        "    full_img_paths[image_files[i]] = img_dir + 'bikelanes/' + image_files[i]\n",
        "\n",
        "# get the end time\n",
        "et = time.time()\n",
        "\n",
        "# get the execution time\n",
        "elapsed_time = et - st\n",
        "print('Execution time:', elapsed_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save embeddings and corresponding image paths for later use"
      ],
      "metadata": {
        "id": "Wnf5jw7sC-wI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1sl61k-Hjazo"
      },
      "outputs": [],
      "source": [
        "# save embeddigs as pickle in main folder\n",
        "file_name = img_dir + '/' + model_name + '_' + dataset + \"_embeddings.pickle\"\n",
        "\n",
        "# Save the dictionary as a pickle file\n",
        "with open(file_name, \"wb\") as file:\n",
        "    pickle.dump(img_embedding, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lloCPRWEjbU7"
      },
      "outputs": [],
      "source": [
        "# save full file paths as pickle too\n",
        "file_name = img_dir + '/' + model_name + '_' + dataset +\"_paths.pickle\"\n",
        "\n",
        "# Save the dictionary as a pickle file\n",
        "with open(file_name, \"wb\") as file:\n",
        "    pickle.dump(full_img_paths, file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa4uyJ9KazLnSTQzGUgdFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}